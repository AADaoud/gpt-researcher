"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[304],{3905:(e,a,t)=>{t.d(a,{Zo:()=>u,kt:()=>d});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function p(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var i=n.createContext({}),s=function(e){var a=n.useContext(i),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},u=function(e){var a=s(e.components);return n.createElement(i.Provider,{value:a},e.children)},c={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,u=p(e,["components","mdxType","originalType","parentName"]),m=s(t),d=r,h=m["".concat(i,".").concat(d)]||m[d]||c[d]||o;return t?n.createElement(h,l(l({ref:a},u),{},{components:t})):n.createElement(h,l({ref:a},u))}));function d(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,l=new Array(o);l[0]=m;var p={};for(var i in a)hasOwnProperty.call(a,i)&&(p[i]=a[i]);p.originalType=e,p.mdxType="string"==typeof e?e:r,l[1]=p;for(var s=2;s<o;s++)l[s]=t[s];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},9598:(e,a,t)=>{t.r(a),t.d(a,{contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>p,toc:()=>i});var n=t(7462),r=(t(7294),t(3905));const o={},l="Configure LLM",p={unversionedId:"gpt-researcher/llms",id:"gpt-researcher/llms",isDocsHomePage:!1,title:"Configure LLM",description:"As described in the introduction, the default LLM is OpenAI due to its superior performance and speed.",source:"@site/docs/gpt-researcher/llms.md",sourceDirName:"gpt-researcher",slug:"/gpt-researcher/llms",permalink:"/docs/gpt-researcher/llms",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tailored Research",permalink:"/docs/gpt-researcher/tailored-research"},next:{title:"LangGraph",permalink:"/docs/gpt-researcher/langgraph"}},i=[{value:"Custom OpenAI",id:"custom-openai",children:[{value:"Custom OpenAI API LLM",id:"custom-openai-api-llm",children:[],level:3},{value:"Custom OpenAI API EMBEDDING",id:"custom-openai-api-embedding",children:[],level:3},{value:"Azure OpenAI",id:"azure-openai",children:[],level:3}],level:2},{value:"Ollama",id:"ollama",children:[],level:2},{value:"Groq",id:"groq",children:[{value:"Sign up",id:"sign-up",children:[],level:3},{value:"Update env vars",id:"update-env-vars",children:[],level:3}],level:2},{value:"Anthropic",id:"anthropic",children:[],level:2},{value:"Mistral",id:"mistral",children:[],level:2},{value:"Together AI",id:"together-ai",children:[],level:2},{value:"HuggingFace",id:"huggingface",children:[],level:2},{value:"Google Gemini",id:"google-gemini",children:[],level:2}],s={toc:i};function u(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},s,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"configure-llm"},"Configure LLM"),(0,r.kt)("p",null,"As described in the ",(0,r.kt)("a",{parentName:"p",href:"/docs/gpt-researcher/config"},"introduction"),", the default LLM is OpenAI due to its superior performance and speed.\nWith that said, GPT Researcher supports various open/closed source LLMs, and you can easily switch between them by adding the ",(0,r.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," env variable and corresponding configuration params.\nCurrent supported LLMs are ",(0,r.kt)("inlineCode",{parentName:"p"},"openai"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"google")," (gemini), ",(0,r.kt)("inlineCode",{parentName:"p"},"azureopenai"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"ollama"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"anthropic"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"mistral"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"huggingface")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"groq"),"."),(0,r.kt)("p",null,"Using any model will require at least updating the ",(0,r.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," param and passing the LLM provider API Key. You might also need to update the ",(0,r.kt)("inlineCode",{parentName:"p"},"SMART_LLM_MODEL")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"FAST_LLM_MODEL")," env vars.\nTo learn more about support customization options see ",(0,r.kt)("a",{parentName:"p",href:"/gpt-researcher/config"},"here"),"."),(0,r.kt)("p",null,"Below you can find examples for how to configure the various supported LLMs."),(0,r.kt)("h2",{id:"custom-openai"},"Custom OpenAI"),(0,r.kt)("p",null,"Create a local OpenAI API using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start"},"llama.cpp Server"),"."),(0,r.kt)("h3",{id:"custom-openai-api-llm"},"Custom OpenAI API LLM"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API LLM provider\nLLM_PROVIDER="openai"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="Your Key"\n\n# specify the custom OpenAI API llm model  \nFAST_LLM_MODEL="gpt-3.5-turbo-16k"\n# specify the custom OpenAI API llm model  \nSMART_LLM_MODEL="gpt-4o"\n\n')),(0,r.kt)("h3",{id:"custom-openai-api-embedding"},"Custom OpenAI API EMBEDDING"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API EMBEDDING provider\nEMBEDDING_PROVIDER="custom"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="Your Key"\n\n# specify the custom OpenAI API embedding model   \nOPENAI_EMBEDDING_MODEL="custom_model"\n')),(0,r.kt)("h3",{id:"azure-openai"},"Azure OpenAI"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'EMBEDDING_PROVIDER="azureopenai"\nAZURE_OPENAI_API_KEY="Your key"\n')),(0,r.kt)("h2",{id:"ollama"},"Ollama"),(0,r.kt)("p",null,"GPT Researcher supports both Ollama LLMs and embeddings. You can choose each or both.\nTo use ",(0,r.kt)("a",{parentName:"p",href:"http://www.ollama.com"},"Ollama")," you can set the following environment variables"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# Use ollama for both, LLM and EMBEDDING provider\nLLM_PROVIDER=ollama\n\n# Ollama endpoint to use\nOLLAMA_BASE_URL=http://localhost:11434\n\n# Specify one of the LLM models supported by Ollama\nFAST_LLM_MODEL=llama3\n# Specify one of the LLM models supported by Ollama \nSMART_LLM_MODEL=llama3 \n# The temperature to use, defaults to 0.55\nTEMPERATURE=0.55\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Optional")," - You can also use ollama for embeddings"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"EMBEDDING_PROVIDER=ollama\n# Specify one of the embedding models supported by Ollama \nOLLAMA_EMBEDDING_MODEL=nomic-embed-text\n")),(0,r.kt)("h2",{id:"groq"},"Groq"),(0,r.kt)("p",null,"GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.\nTo leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (",(0,r.kt)("strong",{parentName:"p"},"NOTE:")," Groq has a very ",(0,r.kt)("em",{parentName:"p"},"generous free tier"),".)"),(0,r.kt)("h3",{id:"sign-up"},"Sign up"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"You can signup here: ",(0,r.kt)("a",{parentName:"p",href:"https://console.groq.com/login"},"https://console.groq.com/login"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Once you are logged in, you can get an API Key here: ",(0,r.kt)("a",{parentName:"p",href:"https://console.groq.com/keys"},"https://console.groq.com/keys"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Once you have an API key, you will need to add it to your ",(0,r.kt)("inlineCode",{parentName:"p"},"systems environment")," using the variable name:\n",(0,r.kt)("inlineCode",{parentName:"p"},'GROQ_API_KEY="*********************"')))),(0,r.kt)("h3",{id:"update-env-vars"},"Update env vars"),(0,r.kt)("p",null,"And finally, you will need to configure the GPT-Researcher Provider and Model variables:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# To use Groq set the llm provider to groq\nLLM_PROVIDER=groq\nGROQ_API_KEY=[Your Key]\n\n# Set one of the LLM models supported by Groq\nFAST_LLM_MODEL=Mixtral-8x7b-32768\n\n# Set one of the LLM models supported by Groq\nSMART_LLM_MODEL=Mixtral-8x7b-32768 \n\n# The temperature to use defaults to 0.55\nTEMPERATURE=0.55\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"NOTE:")," As of the writing of this Doc (May 2024), the available Language Models from Groq are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Llama3-70b-8192"),(0,r.kt)("li",{parentName:"ul"},"Llama3-8b-8192"),(0,r.kt)("li",{parentName:"ul"},"Mixtral-8x7b-32768"),(0,r.kt)("li",{parentName:"ul"},"Gemma-7b-it")),(0,r.kt)("h2",{id:"anthropic"},"Anthropic"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.anthropic.com/"},"Anthropic")," is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=anthropic\nANTHROPIC_API_KEY=[Your key]\n")),(0,r.kt)("p",null,"You can then define the fast and smart LLM models for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"FAST_LLM_MODEL=claude-2.1\nSMART_LLM_MODEL=claude-3-opus-20240229\n")),(0,r.kt)("p",null,"You can then define the fast and smart LLM models for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"FAST_LLM_MODEL=claude-2.1\nSMART_LLM_MODEL=claude-3-opus-20240229\n")),(0,r.kt)("h2",{id:"mistral"},"Mistral"),(0,r.kt)("p",null,"Sign up for a ",(0,r.kt)("a",{parentName:"p",href:"https://console.mistral.ai/users/api-keys/"},"Mistral API key"),".\nThen update the corresponding env vars, for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=mistral\nANTHROPIC_API_KEY=[Your key]\nFAST_LLM_MODEL=open-mistral-7b\nSMART_LLM_MODEL=mistral-large-latest\n")),(0,r.kt)("h2",{id:"together-ai"},"Together AI"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.together.ai/"},"Together AI")," offers an API to query ",(0,r.kt)("a",{parentName:"p",href:"https://docs.together.ai/docs/inference-models"},"50+ leading open-source models")," in a couple lines of code.\nThen update corresponding env vars, for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=together\nTOGETHER_API_KEY=[Your key]\nFAST_LLM_MODEL=meta-llama/Llama-3-8b-chat-hf\nSMART_LLM_MODEL=meta-llama/Llama-3-70b-chat-hf\n")),(0,r.kt)("h2",{id:"huggingface"},"HuggingFace"),(0,r.kt)("p",null,"This integration requires a bit of extra work. Follow ",(0,r.kt)("a",{parentName:"p",href:"https://python.langchain.com/v0.1/docs/integrations/chat/huggingface/"},"this guide")," to learn more.\nAfter you've followed the tutorial above, update the env vars:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=huggingface\nHUGGINGFACE_API_KEY=[Your key]\nFAST_LLM_MODEL=HuggingFaceH4/zephyr-7b-beta\nSMART_LLM_MODEL=HuggingFaceH4/zephyr-7b-beta\n")),(0,r.kt)("h2",{id:"google-gemini"},"Google Gemini"),(0,r.kt)("p",null,"Sign up ",(0,r.kt)("a",{parentName:"p",href:"https://ai.google.dev/gemini-api/docs/api-key"},"here")," for obtaining a Google Gemini API Key and update the following env vars:"),(0,r.kt)("p",null,"Please make sure to update fast and smart models to corresponding valid Gemini models."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"LLM_PROVIDER=google\nGEMINI_API_KEY=[Your key]\n")))}u.isMDXComponent=!0}}]);