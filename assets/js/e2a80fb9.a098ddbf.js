"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[304],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>d});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var p=n.createContext({}),s=function(e){var t=n.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},c=function(e){var t=s(e.components);return n.createElement(p.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,p=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),m=s(r),d=a,h=m["".concat(p,".").concat(d)]||m[d]||u[d]||o;return r?n.createElement(h,l(l({ref:t},c),{},{components:r})):n.createElement(h,l({ref:t},c))}));function d(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,l=new Array(o);l[0]=m;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i.mdxType="string"==typeof e?e:a,l[1]=i;for(var s=2;s<o;s++)l[s]=r[s];return n.createElement.apply(null,l)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},9598:(e,t,r)=>{r.r(t),r.d(t,{contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var n=r(7462),a=(r(7294),r(3905));const o={},l="Configure LLM",i={unversionedId:"gpt-researcher/llms",id:"gpt-researcher/llms",isDocsHomePage:!1,title:"Configure LLM",description:"As described in the introduction, the default LLM is OpenAI due to its superior performance and speed.",source:"@site/docs/gpt-researcher/llms.md",sourceDirName:"gpt-researcher",slug:"/gpt-researcher/llms",permalink:"/docs/gpt-researcher/llms",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tailored Research",permalink:"/docs/gpt-researcher/tailored-research"},next:{title:"LangGraph x GPT Researcher",permalink:"/docs/gpt-researcher/langgraph"}},p=[{value:"OpenAI",id:"openai",children:[{value:"custom OpenAI API LLM",id:"custom-openai-api-llm",children:[],level:3},{value:"custom OpenAI API EMBEDDING",id:"custom-openai-api-embedding",children:[],level:3}],level:2},{value:"Ollama",id:"ollama",children:[],level:2},{value:"Groq",id:"groq",children:[],level:2},{value:"Anthropic",id:"anthropic",children:[],level:2}],s={toc:p};function c(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},s,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"configure-llm"},"Configure LLM"),(0,a.kt)("p",null,"As described in the ",(0,a.kt)("a",{parentName:"p",href:"/docs/gpt-researcher/config"},"introduction"),", the default LLM is OpenAI due to its superior performance and speed.\nHowever, GPT Researcher supports various open/closed source LLMs, and you can easily switch between them by adding the ",(0,a.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," env variable and corresponding configuration params."),(0,a.kt)("p",null,"Below you can find how to configure the various supported LLMs."),(0,a.kt)("h2",{id:"openai"},"OpenAI"),(0,a.kt)("p",null,"Create a local OpenAI API using ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start"},"llama.cpp Server"),"."),(0,a.kt)("h3",{id:"custom-openai-api-llm"},"custom OpenAI API LLM"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API LLM provider\nLLM_PROVIDER="openai"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="custom_key"\n\n# specify the custom OpenAI API llm model  \nFAST_LLM_MODEL="gpt-3.5-turbo-16k"\n# specify the custom OpenAI API llm model  \nSMART_LLM_MODEL="gpt-4o"\n\n')),(0,a.kt)("h3",{id:"custom-openai-api-embedding"},"custom OpenAI API EMBEDDING"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API EMBEDDING provider\nEMBEDDING_PROVIDER="custom"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="custom_key"\n\n# specify the custom OpenAI API embedding model   \nOPENAI_EMBEDDING_MODEL="custom_model"\n')),(0,a.kt)("h2",{id:"ollama"},"Ollama"),(0,a.kt)("h2",{id:"groq"},"Groq"),(0,a.kt)("p",null,"GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.\nTo leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (",(0,a.kt)("strong",{parentName:"p"},"NOTE:")," Groq has a very ",(0,a.kt)("em",{parentName:"p"},"generous free tier"),".)"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"You can signup here: ",(0,a.kt)("a",{parentName:"p",href:"https://console.groq.com/login"},"https://console.groq.com/login"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Once you are logged in, you can get an API Key here: ",(0,a.kt)("a",{parentName:"p",href:"https://console.groq.com/keys"},"https://console.groq.com/keys"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Once you have an API key, you will need to add it to your ",(0,a.kt)("inlineCode",{parentName:"p"},"systems environment")," using the variable name:\n",(0,a.kt)("inlineCode",{parentName:"p"},'GROQ_API_KEY="*********************"')))),(0,a.kt)("p",null,"And finally, you will need to configure the GPT-Researcher Provider and Model variables:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"# To use Groq set the llm provider to groq\nLLM_PROVIDER=groq\n\n# Set one of the LLM models supported by Groq\nFAST_LLM_MODEL=Mixtral-8x7b-32768\n\n# Set one of the LLM models supported by Groq\nSMART_LLM_MODEL=Mixtral-8x7b-32768 \n\n# The temperature to use defaults to 0.55\nTEMPERATURE=0.55\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"NOTE:")," As of the writing of this Doc (May 2024), the available Language Models from Groq are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Llama3-70b-8192"),(0,a.kt)("li",{parentName:"ul"},"Llama3-8b-8192"),(0,a.kt)("li",{parentName:"ul"},"Mixtral-8x7b-32768"),(0,a.kt)("li",{parentName:"ul"},"Gemma-7b-it")),(0,a.kt)("h2",{id:"anthropic"},"Anthropic"),(0,a.kt)("p",null,"..."))}c.isMDXComponent=!0}}]);