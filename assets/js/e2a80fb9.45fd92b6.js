"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[304],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),i=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=i(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),m=i(n),d=a,h=m["".concat(s,".").concat(d)]||m[d]||u[d]||o;return n?r.createElement(h,l(l({ref:t},c),{},{components:n})):r.createElement(h,l({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,l=new Array(o);l[0]=m;var p={};for(var s in t)hasOwnProperty.call(t,s)&&(p[s]=t[s]);p.originalType=e,p.mdxType="string"==typeof e?e:a,l[1]=p;for(var i=2;i<o;i++)l[i]=n[i];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9598:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>p,toc:()=>s});var r=n(7462),a=(n(7294),n(3905));const o={},l="Configure LLM",p={unversionedId:"gpt-researcher/llms",id:"gpt-researcher/llms",isDocsHomePage:!1,title:"Configure LLM",description:"As described in the introduction, the default LLM is OpenAI due to its superior performance and speed.",source:"@site/docs/gpt-researcher/llms.md",sourceDirName:"gpt-researcher",slug:"/gpt-researcher/llms",permalink:"/docs/gpt-researcher/llms",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Tailored Research",permalink:"/docs/gpt-researcher/tailored-research"},next:{title:"LangGraph",permalink:"/docs/gpt-researcher/langgraph"}},s=[{value:"OpenAI",id:"openai",children:[{value:"custom OpenAI API LLM",id:"custom-openai-api-llm",children:[],level:3},{value:"custom OpenAI API EMBEDDING",id:"custom-openai-api-embedding",children:[],level:3}],level:2},{value:"Ollama",id:"ollama",children:[],level:2},{value:"Groq",id:"groq",children:[],level:2},{value:"Anthropic",id:"anthropic",children:[],level:2}],i={toc:s};function c(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},i,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"configure-llm"},"Configure LLM"),(0,a.kt)("p",null,"As described in the ",(0,a.kt)("a",{parentName:"p",href:"/docs/gpt-researcher/config"},"introduction"),", the default LLM is OpenAI due to its superior performance and speed.\nHowever, GPT Researcher supports various open/closed source LLMs, and you can easily switch between them by adding the ",(0,a.kt)("inlineCode",{parentName:"p"},"LLM_PROVIDER")," env variable and corresponding configuration params."),(0,a.kt)("p",null,"Below you can find how to configure the various supported LLMs."),(0,a.kt)("h2",{id:"openai"},"OpenAI"),(0,a.kt)("p",null,"Create a local OpenAI API using ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start"},"llama.cpp Server"),"."),(0,a.kt)("h3",{id:"custom-openai-api-llm"},"custom OpenAI API LLM"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API LLM provider\nLLM_PROVIDER="openai"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="custom_key"\n\n# specify the custom OpenAI API llm model  \nFAST_LLM_MODEL="gpt-3.5-turbo-16k"\n# specify the custom OpenAI API llm model  \nSMART_LLM_MODEL="gpt-4o"\n\n')),(0,a.kt)("h3",{id:"custom-openai-api-embedding"},"custom OpenAI API EMBEDDING"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'# use a custom OpenAI API EMBEDDING provider\nEMBEDDING_PROVIDER="custom"\n\n# set the custom OpenAI API url\nOPENAI_BASE_URL="http://localhost:1234/v1"\n# set the custom OpenAI API key\nOPENAI_API_KEY="custom_key"\n\n# specify the custom OpenAI API embedding model   \nOPENAI_EMBEDDING_MODEL="custom_model"\n')),(0,a.kt)("h2",{id:"ollama"},"Ollama"),(0,a.kt)("p",null,"To use ",(0,a.kt)("a",{parentName:"p",href:"http://www.ollama.com"},"Ollama")," you have to set the following environment variables"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"# use ollama for both, LLM and EMBEDDING provider\nLLM_PROVIDER=ollama\nEMBEDDING_PROVIDER=ollama\n\n# the Ollama endpoint to use\nOLLAMA_BASE_URL=http://localhost:11434\n\n# specify one of the LLM models supported by Ollama\nFAST_LLM_MODEL=llama3\n# specify one of the LLM models supported by Ollama \nSMART_LLM_MODEL=llama3 \n# the temperature to use, defaults to 0.55\nTEMPERATURE=0.55\n\n# specify one of the embedding models supported by Ollama \nOLLAMA_EMBEDDING_MODEL=nomic-embed-text\n")),(0,a.kt)("h2",{id:"groq"},"Groq"),(0,a.kt)("p",null,"GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.\nTo leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (",(0,a.kt)("strong",{parentName:"p"},"NOTE:")," Groq has a very ",(0,a.kt)("em",{parentName:"p"},"generous free tier"),".)"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"You can signup here: ",(0,a.kt)("a",{parentName:"p",href:"https://console.groq.com/login"},"https://console.groq.com/login"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Once you are logged in, you can get an API Key here: ",(0,a.kt)("a",{parentName:"p",href:"https://console.groq.com/keys"},"https://console.groq.com/keys"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Once you have an API key, you will need to add it to your ",(0,a.kt)("inlineCode",{parentName:"p"},"systems environment")," using the variable name:\n",(0,a.kt)("inlineCode",{parentName:"p"},'GROQ_API_KEY="*********************"')))),(0,a.kt)("p",null,"And finally, you will need to configure the GPT-Researcher Provider and Model variables:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"# To use Groq set the llm provider to groq\nLLM_PROVIDER=groq\n\n# Set one of the LLM models supported by Groq\nFAST_LLM_MODEL=Mixtral-8x7b-32768\n\n# Set one of the LLM models supported by Groq\nSMART_LLM_MODEL=Mixtral-8x7b-32768 \n\n# The temperature to use defaults to 0.55\nTEMPERATURE=0.55\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"NOTE:")," As of the writing of this Doc (May 2024), the available Language Models from Groq are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Llama3-70b-8192"),(0,a.kt)("li",{parentName:"ul"},"Llama3-8b-8192"),(0,a.kt)("li",{parentName:"ul"},"Mixtral-8x7b-32768"),(0,a.kt)("li",{parentName:"ul"},"Gemma-7b-it")),(0,a.kt)("h2",{id:"anthropic"},"Anthropic"),(0,a.kt)("p",null,"..."))}c.isMDXComponent=!0}}]);