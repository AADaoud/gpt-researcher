"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1842],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},f={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=o,m=u["".concat(l,".").concat(d)]||u[d]||f[d]||a;return n?r.createElement(m,i(i({ref:t},p),{},{components:n})):r.createElement(m,i({ref:t},p))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<a;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7690:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=n(7462),o=(n(7294),n(3905));const a={},i="Introduction",s={unversionedId:"gpt-researcher/config",id:"gpt-researcher/config",isDocsHomePage:!1,title:"Introduction",description:"The config.py enables you to customize GPT Researcher to your specific needs and preferences.",source:"@site/docs/gpt-researcher/config.md",sourceDirName:"gpt-researcher",slug:"/gpt-researcher/config",permalink:"/docs/gpt-researcher/config",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/config.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Troubleshooting",permalink:"/docs/gpt-researcher/troubleshooting"},next:{title:"Tailored Research",permalink:"/docs/gpt-researcher/tailored-research"}},l=[],c={toc:l};function p(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"The config.py enables you to customize GPT Researcher to your specific needs and preferences."),(0,o.kt)("p",null,"Thanks to our amazing community and contributions, GPT Researcher supports multiple LLMs and Retrievers.\nIn addition, GPT Researcher can be tailored to various report formats (such as APA), word count, research iterations depth, etc."),(0,o.kt)("p",null,"GPT Researcher defaults to our recommended suite of integrations: ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/overview"},"OpenAI")," for LLM calls and ",(0,o.kt)("a",{parentName:"p",href:"https://app.tavily.com"},"Tavily API")," for retrieving realtime online information."),(0,o.kt)("p",null,"As seen below, OpenAI still stands as the superior LLM. We assume it will stay this way for some time, and that prices will only continue to decrease, while performance and speed increase over time."),(0,o.kt)("div",{style:{marginBottom:"10px"}},(0,o.kt)("img",{align:"center",height:"350",src:"/img/leaderboard.png"})),(0,o.kt)("p",null,"Here is an example of the default config.py file found in ",(0,o.kt)("inlineCode",{parentName:"p"},"/gpt_researcher/config/"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import os\ndef __init__(self, config_file: str = None):\n    \"\"\"Initialize the config class.\"\"\"\n    self.config_file = os.path.expanduser(config_file) if config_file else os.getenv('CONFIG_FILE')\n    self.retriever = os.getenv('RETRIEVER', \"tavily\")\n    self.embedding_provider = os.getenv('EMBEDDING_PROVIDER', 'openai')\n    self.llm_provider = os.getenv('LLM_PROVIDER', \"openai\")\n    self.fast_llm_model = os.getenv('FAST_LLM_MODEL', \"gpt-3.5-turbo-16k\")\n    self.smart_llm_model = os.getenv('SMART_LLM_MODEL', \"gpt-4o\")\n    self.fast_token_limit = int(os.getenv('FAST_TOKEN_LIMIT', 2000))\n    self.smart_token_limit = int(os.getenv('SMART_TOKEN_LIMIT', 4000))\n    self.browse_chunk_max_length = int(os.getenv('BROWSE_CHUNK_MAX_LENGTH', 8192))\n    self.summary_token_limit = int(os.getenv('SUMMARY_TOKEN_LIMIT', 700))\n    self.temperature = float(os.getenv('TEMPERATURE', 0.55))\n    self.user_agent = os.getenv('USER_AGENT', \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n                                               \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\")\n    self.max_search_results_per_query = int(os.getenv('MAX_SEARCH_RESULTS_PER_QUERY', 5))\n    self.memory_backend = os.getenv('MEMORY_BACKEND', \"local\")\n    self.total_words = int(os.getenv('TOTAL_WORDS', 800))\n    self.report_format = os.getenv('REPORT_FORMAT', \"APA\")\n    self.max_iterations = int(os.getenv('MAX_ITERATIONS', 3))\n    self.agent_role = os.getenv('AGENT_ROLE', None)\n    self.scraper = os.getenv(\"SCRAPER\", \"bs\")\n    self.max_subtopics = os.getenv(\"MAX_SUBTOPICS\", 3)\n    self.doc_path = os.getenv(\"DOC_PATH\", \"\")\n")),(0,o.kt)("p",null,"To change the default configurations, you can simply add env variables to your ",(0,o.kt)("inlineCode",{parentName:"p"},".env")," file as named in the config.py file."),(0,o.kt)("p",null,"Please note that you can also include your own external JSON file ",(0,o.kt)("inlineCode",{parentName:"p"},"config.json")," by adding the path in the ",(0,o.kt)("inlineCode",{parentName:"p"},"config_file")," param."),(0,o.kt)("p",null,"To learn more about additional LLM support you can check out the docs ",(0,o.kt)("a",{parentName:"p",href:"/docs/gpt-researcher/llms"},"here"),"."),(0,o.kt)("p",null,"You can also change the search engine by modifying the ",(0,o.kt)("inlineCode",{parentName:"p"},"retriever")," param to others such as ",(0,o.kt)("inlineCode",{parentName:"p"},"duckduckgo"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"bing"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"google"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"serper"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"searx")," and more. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/assafelovic/gpt-researcher/tree/master/gpt_researcher/retrievers"},"Check here")," for supported retrievers."),(0,o.kt)("p",null,"Please note that you might need to sign up and obtain an API key for any of the other supported retrievers and LLM providers."))}p.isMDXComponent=!0}}]);